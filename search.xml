<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[阿里云RDS本地恢复至MySQL]]></title>
    <url>%2F2018%2F11%2F08%2F%E9%98%BF%E9%87%8C%E4%BA%91RDS%E6%9C%AC%E5%9C%B0%E6%81%A2%E5%A4%8D%2F</url>
    <content type="text"><![CDATA[阿里云RDS数据恢复至本地MySQL数据库本地MySQL数据库版本必须和阿里云RDSMySQL数据库版本一致 参考自恢复云数据库MySQL的备份文件到自建数据库 1、安装 percona-Xtrabackup RDS MySQL使用percona-Xtrabackup对MySQL数据库进行备份。所以使用该软件对数据进行恢复该软件版本依据阿里云RDS的MySQL版本而定，阿里云MySQL版本5.6及以前版本，使用2.3版本；5.7及以后使用2.4或以上版本。具体请参考阿里云官网。 percona-Xtrabackup2.3 percona-Xtrabackup2.4 以上链接中包含有具体安装指导。最简单的方式为第二种，下载对应版本的rpm包，使用yum install xxx.rpm安装。 2、解压缩下载的备份文件1、下载备份文件：wget -c &#39;待下载备份的外网链接&#39; -O 自定义文件名.tar.gz 对于tar压缩文件(.tar.gz)，使用命令： tar -izxvf test.tar.gz -C /data/msyql 对于xbstream 压缩包(.xb.gz)，使用命令： gzip -d -c test.xb.gz | xbstream -x -v -C /data/mysql 以上 ‘test.tar.gz 和test.xb.gz均为下载下来的本份文件’，/data/mysql目录将作为本地MySQL的新的datadir目录，需事先创建好。 2、使用一下命令恢复解压好的备份文件 innobackupex --defaults-file=/data/mysql/backup-my.cnf --user=root --apply-log /data/mysql/ 看到如下输出，则恢复完成： 3、打开/data/mysql/backup-my.cnf，注释掉如下参数： 4、修改/data/mysql所属用户和组 chown -R mysql:mysql /data/mysql 5、启动MySQL进程 mysqld_safe --defaults-file=/data/mysql/backup-my.cnf --user=mysql --datadir=/data/mysql/ &amp; 6、查看进程 ps -ef |grep &#39;mysqld&#39; 3、登陆数据库mysql -uroot 直接登录，会发现登录失败，可在启动MySQL服务时，添加--skip-grant-tables,以跳过密码验证。（网上都说RDS备份的数据库，没有密码，我没发现，所以有此操作）]]></content>
      <categories>
        <category>MySQL</category>
      </categories>
      <tags>
        <tag>MySQL5.6</tag>
        <tag>阿里云RDS</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Nginx的安装与静态网站部署]]></title>
    <url>%2F2018%2F11%2F04%2FNginx%E5%8F%8D%E5%90%91%E4%BB%A3%E7%90%86%E4%B8%8E%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1%2F</url>
    <content type="text"><![CDATA[Nginx的安装与静态网站部署Nginx简介 Nginx是一款高性能的HTTP服务器/反向代理服务器及电子邮件服务器。官方测试能够支撑5万并发访问，并且CPU、内存等资源消耗却非常低，运行非常稳定。 Nginx应用场景 HTTP服务器。Nginx可以做网页静态服务器。 虚拟主机。 可以实现在一台服务器虚拟出多个网站。 反向代理，负载均衡。当单台服务器不能满足用户的请求时，需要用多台服务器集群可以使用 nginx做反向代理。并且多台服务器可以平均分担负载，不会因为某台服务器负载过高宕机而某台服务器闲置的情况。 Nginx在Linux下的安装 环境：gcc-c++，pcre，pcre-devel，zlib，zlib-devel，openssl，openssl-devel 1、上传并解压缩 tar zxvf nginx-1.8.0.tar.gz 2、进入nginx-1.8.0目录，使用configure命令创建MakeFile文件123456789101112./configure \--prefix=/usr/local/nginx \--pid-path=/var/run/nginx/nginx.pid \--lock-path=/var/lock/nginx.lock \--error-log-path=/var/log/nginx/error.log \--http-log-path=/var/log/nginx/access.log \--with-http_gzip_static_module \--http-client-body-temp-path=/var/temp/nginx/client \--http-proxy-temp-path=/var/temp/nginx/proxy \--http-fastcgi-temp-path=/var/temp/nginx/fastcgi \--http-uwsgi-temp-path=/var/temp/nginx/uwsgi \--http-scgi-temp-path=/var/temp/nginx/scgi configure 参数详解：12345678910111213141516171819202122232425./configure \--prefix=/usr \ 指向安装目录--sbin-path=/usr/sbin/nginx \ 指向（执行） 程序文件（nginx）--conf-path=/etc/nginx/nginx.conf \ 指向配置文件--error-log-path=/var/log/nginx/error.log \ 指向 log--http-log-path=/var/log/nginx/access.log \ 指向 http-log--pid-path=/var/run/nginx/nginx.pid \ 指向 pid--lock-path=/var/lock/nginx.lock \ （安装文件锁定， 防止安装文件被别人利用， 或自己误操作。 ）--user=nginx \--group=nginx \--with-http_ssl_module \ 启用 ngx_http_ssl_module 支持（使支持 https 请求， 需已安装openssl）--with-http_flv_module \ 启用 ngx_http_flv_module 支持（提供寻求内存使用基于时间的偏移量文件）--with-http_stub_status_module \ 启用 ngx_http_stub_status_module 支持（获取 nginx 自上次启动以来的工作状态）--with-http_gzip_static_module \ 启用 ngx_http_gzip_static_module 支持（在线实时压缩输出数据流）--http-client-body-temp-path=/var/tmp/nginx/client/ \ 设定 http 客户端请求临时文件路径--http-proxy-temp-path=/var/tmp/nginx/proxy/ \ 设定 http 代理临时文件路径--http-fastcgi-temp-path=/var/tmp/nginx/fcgi/ \ 设定 http fastcgi 临时文件路径--http-uwsgi-temp-path=/var/tmp/nginx/uwsgi \ 设定 http uwsgi 临时文件路径--http-scgi-temp-path=/var/tmp/nginx/scgi \ 设定 http scgi 临时文件路径--with-pcre 启用 pcre 库 3、编译 make -j 8参数 -j 用于指定编译的线程的个数。 4、安装 make install 5、创建启动参数需要的文件目录/var/temp/nginx/client mkdir /var/temp/nginx/client -p Nginx的相关命令 启动nginx :./nginx 关闭nginx :./nginx -s quit 重启nginx :./nginx -s reload Nginx虚拟主机的配置1、上传静态网站 将前端静态页cart.html以及图片样式等资源，上传至 /usr/local/nginx/cart 2、修改Nginx的修改配置文件：/usr/local/nginx/conf/nginx.conf123456789101112131415server &#123; listen 81; # 对外访问端口 server_name localhost; location / &#123; root cart; index cart.html; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html&#123; root html; &#125; &#125; 3、域名绑定 3.1 修改nginx.conf123456789101112131415server &#123; listen 81; # 对外访问端口 server_name cart.pinyougou.com; location / &#123; root cart; index cart.html; &#125; error_page 500 502 503 504 /50x.html; location = /50x.html&#123; root html; &#125; &#125; 3.2、修改本机hosts映射1127.0.0.1 cart.pinyougou.com Nginx反向代理与负载均衡 反向代理(Reverse Proxy):是指以代理服务器来接受Internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给Internet上的请求连接的客户端，此时代理服务器对外就表现为一个反向代理服务器。正向代理针对的是客户端，而反向代理针对的是服务器。 配置反反向代理1、在Nginx主机修改Nginx配置文件12345678910111213upstream tomcat-cart&#123; server 192.168.25.135:9102;&#125;server &#123; listen 80; server_name cart.pinyougou.com; location / &#123; proxy_pass http://tomcat-cart; index index.html index.htm; &#125;&#125; 2、配置域名解析 3、重启nginx,然后使用域名访问 http://cart.pinyougou.com/cart/index.html 负载均衡 &emsp;&emsp; 负载均衡(Load Balance),其意思就是分摊到多个操作单元上进行执行，例如Web服务器、FTP服务器、企业关键应用服务器和其他关键服务器等，从而共同完成工作任务。它建立在现有网络结构上，提供了一种廉价有效透明的方法扩展网络设备和服务器带宽、增加吞吐量加强网络数据处理能力、加强网络数据处理能力、提高网络的灵活性和可用性。 负载均衡的配置(示例)1、部署两个tomcat服务，端口分别为9801和9802. 2、部署网站首页12[root@localhost ~]# cp -r portal/* /usr/local/tomcat-cluster/tomcat-portal-1/webapps/ROOT[root@localhost ~]# cp -r portal/* /usr/local/tomcat-cluster/tomcat-portal-2/webapps/ROOT 3、配置负载均衡 3.1、设置域名指向192.168.25.135 www.pinyougou.com 3.2、修改Nginx配置文件：1234567891011121314upstream tomcat-portal&#123; #配置服务列表，轮询调度以实现负载均衡,**weight**设置服务被调用的权重 server 192.168.25.135:9801 weight=2; server 192.168.25.135:9802;&#125;server &#123; listen 80; server_name www.pinyougou.com; location / &#123; proxy_pass http://tomcat-portal; index index.html; &#125;&#125;]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[keepalived入门]]></title>
    <url>%2F2018%2F10%2F04%2FKeepalived%E5%85%A5%E9%97%A8%2F</url>
    <content type="text"><![CDATA[keepalived入门keepalived简介 &emsp;&emsp;keepalived是集群管理中保证集群高可用的一个服务软件，用来防止单点故障。 &emsp;&emsp;keepalived的作用是检测web服务器的状态，如果有一台web服务器死机，或工作出现故障，keepalived将检测到，并将有故障的web服务器从系统中剔除，当web服务器工作正常后，keepalived自动将web服务器加入到服务器集群中，这些工作全部自动完成，不需要人工干涉，需要人工做的只是修复故障的web服务器。 &emsp;&emsp;keepalived是以VRRP协议位实现基础的，VRRP全称 Virtual RouterRedundancy Protocol，即虚拟路由冗余协议。 &emsp;&emsp;虚拟路由冗余协议，可以认为是实现路由器高可用的协议，即将N台提供相同功能的路由器组成的一个路由器组，这个组里面有一个master和多个backup，master上面有一个对外提供服务的VIP（Virtual IPAddress,虚拟IP地址，该路由器所在局域网内其他机器的默认路由为该VIP），master会发组播，当backup收不到VRRP包时，就认为master宕掉了，这时就需要根据VRRP的优先级来就可以保证路由器的高可用了。 keepalived核心内容 &emsp;&emsp;keepalived主要有三个模块，分别是core、check和VRRP。core模块为keepalived的核心，负责主进程的启动、维护以及全局配置文件的加载和解析。check负责健康检查，包括常见的各种检查。VRRP模块是来实现VRRP协议的。 初始状态： 主机状态： 主机恢复：]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>keepalived</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hadoop-2.7.2高可用集群搭建(Ubuntu14.04)]]></title>
    <url>%2F2017%2F06%2F12%2FHadoop-2-7-2%E9%AB%98%E5%8F%AF%E7%94%A8%E9%9B%86%E7%BE%A4%E6%90%AD%E5%BB%BA%2F</url>
    <content type="text"><![CDATA[Hadoop-2.7.2高可用集群搭建(Ubuntu14.04) 以root用户身份进行配置 环境需求： JDK版本：1.7+ Hadoop版本：hadoop-2.7.2 zookeeper版本：zookeeper-3.4.5 详细配置 主机名IP Namenode Datanode Yarn Zookeeper JournalNode node-1:192.168.111.130 是 是 否 是 是 node-2:192.168.111.129 是 是 是 是 是 node-3:192.168.111.128 否 是 是 是 是 1、安装JDK (1) 下载 jdk-7u80-linux-x64.tar 文件 (2) 解压缩文件 tar -zxvf jdk-7u80-linux-x64.tar(尽量使用jdk1.7) (3) 在 /etc/profile文件末尾，添加jdk的配置信息 (4) 重新加载文件，使配置生效：#source /etc/profile 2、配置SSH免密登陆 SSH免密钥原理： 我们使用ssh-keygen在ServerA上生成私钥跟公钥，将生成的公钥拷贝到远程机器ServerB上后,就可以使用ssh命令无需密码登录到另外一台机器ServerB上。生成公钥与私钥有两种加密方式，第一种是rsa(默认)，还有一种是dsa,使用时两种方式随便选一种即可 (1) 安装SSH服务 1# apt-get install ssh (2) 在三个节点上分别都使用rsa加密生成密钥对 1# ssh-keygen -t rsa 按3次Enter键后，会在/root/.ssh/下生成id_rsa (私钥)，和id_rsa.pub(公钥) (3) 将node-1上，将id_rsa.pub读取到authorized_keys中(authorized_keys 第一次读入是不存在的) 1# cat ~/.ssh/id_rsa.pub &gt;&gt; ~/.ssh/authorized_keys (4) 分别将node-2和node-3上的公钥都读入到master的authorized_keys中 在node-2和node-3上： #scp /root/.ssh/id_rsa.pub master:/root/ 在node-1上： 每收到一个公钥都读取一次(一个一个地写入文件)： # cat /root/id_rsa.pub &gt;&gt; /root/.ssh/authorized_keys (5) 将node-1上的authorized_keys发送到node-2和node-3，以实现三个节点之间的免密钥登录 12#scp /root/.ssh/authorized_keys node-2:/root/.ssh/#scp /root/.ssh/authorized_keys node-3:/root/.ssh/ (6) 在3个节点上修改authorized_keys的权限： 1# chmod 600 ~/.ssh/authorized_keys 3、Zookeeper集群搭建 (1) 下载zookeeper-3.4.9.tar.gz (2) 解压 1#tar -zxvf zookeeper-3.4.9.tar.gz -C /home/ (3) 修改zoo.cfg（可以使用zoo_sample.cfg来生成zoo.cfg） 12# cp zoo_sample.cfg zoo.cfg# vim zoo.cfg 参数说明: tickTime: zookeeper中使用的基本时间单位, 毫秒值. dataDir: 数据目录. 可以是任意目录. dataLogDir: log目录, 同样可以是任意目录. 如果没有设置该参数, 将使用和dataDir相同的设置. clientPort: 监听client连接的端口号. server.X=A:B:C 其中X是一个数字, 表示这是第几号server. A是该server所在的IP地址. B配置该server和集群中的leader交换消息所使用的端口. C配置选举leader时所使用的端口. 由于配置的是伪集群模式, 所以各个server的B, C参数必须不同. (4). 根据zoo.cfg各自在节点上的 dataDir下添加myid node-1： #echo &quot;1&quot; &gt;&gt; /home/zookeeperData/myid node-2: #echo &quot;2&quot; &gt;&gt; /home/zookeeperData/myid node-3: #echo &quot;3&quot; &gt;&gt; /home/zookeeperData/myid (5) 在各个节点上开启zookeeper服务,并且查看服务是否成功启动 12#zkServer.sh start#zkServer.sh status 4、Hadoop集群的搭建 (1) 下载 hadoop-2.7.2.tar.gz,并解压 1tar -zxvf hadoop-2.7.2.tar.gz -C /home/ (2)修改如下配置文件(在/home/hadoop-2.7.2/etc/hadoop/下) 修改core-site.xml 123456789101112131415161718&lt;configuration&gt; &lt;property&gt; &lt;name&gt;fs.defaultFS&lt;/name&gt; &lt;value&gt;hdfs://nameserver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;hadoop.tmp.dir&lt;/name&gt; &lt;value&gt;/home/hdfs_all/tmp/&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;io.file.buffer.size&lt;/name&gt; &lt;value&gt;1024&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;ha.zookeeper.quorum&lt;/name&gt; &lt;value&gt;node-1:2181,node-2:2181,node-3:2181&lt;/value&gt; &lt;/property&gt; &lt;/configuration&gt; 修改hdfs-site.xml 1234567891011121314151617181920212223242526272829303132333435363738394041424344454647484950515253545556575859606162&lt;configuration&gt; &lt;property&gt; &lt;name&gt;dfs.nameservices&lt;/name&gt; &lt;value&gt;nameserver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.namenodes.nameserver&lt;/name&gt; &lt;value&gt;nn1,nn2&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.nameserver.nn1&lt;/name&gt; &lt;value&gt;node-1:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.nameserver.nn1&lt;/name&gt; &lt;value&gt;node-1:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.rpc-address.nameserver.nn2&lt;/name&gt; &lt;value&gt;node-2:9000&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.http-address.nameserver.nn2&lt;/name&gt; &lt;value&gt;node-2:50070&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.shared.edits.dir&lt;/name&gt; &lt;value&gt;qjournal://node-1:8485;node-2:8485;node-3:8485/nameserver&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.journalnode.edits.dir&lt;/name&gt; &lt;value&gt;/home/hdfs_all/journal&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.automatic-failover.enabled&lt;/name&gt; &lt;value&gt;true&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.client.failover.proxy.provider.nameserver&lt;/name&gt; &lt;value&gt;org.apache.hadoop.hdfs.server.namenode.ha.ConfiguredFailoverProxyProvider&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.methods&lt;/name&gt; &lt;value&gt;sshfence&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.ha.fencing.ssh.private-key-files&lt;/name&gt; &lt;value&gt;/root/.ssh/id_rsa&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.namenode.name.dir&lt;/name&gt; &lt;value&gt;file:///home/hdfs_all/hdfs/name&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.datanode.data.dir&lt;/name&gt; &lt;value&gt;file:///home/hdfs_all/hdfs/data&lt;/value&gt; &lt;/property&gt; &lt;property&gt; &lt;name&gt;dfs.replication&lt;/name&gt; &lt;value&gt;2&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改mapred-site.xml(刚解压的文件中没有给文件，将mapred-site-tempalte.xml复制一份即可) 123456&lt;configuration&gt; &lt;property&gt; &lt;name&gt;mapreduce.framework.name&lt;/name&gt; &lt;value&gt;yarn&lt;/value&gt; &lt;/property&gt;&lt;/configuration&gt; 修改yarn-site.xml 123456789101112131415161718192021222324252627282930313233343536 &lt;configuration&gt;&lt;!-- 开启RM高可用 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.ha.enabled&lt;/name&gt;&lt;value&gt;true&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定RM的cluster id --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.cluster-id&lt;/name&gt;&lt;value&gt;yrc&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定RM的名字 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.ha.rm-ids&lt;/name&gt;&lt;value&gt;rm1,rm2&lt;/value&gt;&lt;/property&gt;&lt;!-- 分别指定RM的地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname.rm1&lt;/name&gt;&lt;value&gt;node-2&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.hostname.rm2&lt;/name&gt;&lt;value&gt;node-3&lt;/value&gt;&lt;/property&gt;&lt;!-- 指定zk集群地址 --&gt;&lt;property&gt;&lt;name&gt;yarn.resourcemanager.zk-address&lt;/name&gt;&lt;value&gt;node-1:2181,node-2:2181,node-3:2181&lt;/value&gt;&lt;/property&gt;&lt;property&gt;&lt;name&gt;yarn.nodemanager.aux-services&lt;/name&gt;&lt;value&gt;mapreduce_shuffle&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt; 修改slaves 删除localhost,添加如下内容： 123node-1node-2node-3 修改JAVA_HOME 分别在文件hadoop-env.sh和yarn-env.sh中添加JAVA_HOME配置 5、将hadoop配置文件发送至另外两个节点12#scp -r /home/hadoop-2.7.2 node-2:/home/#scp -r /home/hadoop-2.7.2 node-3:/home/ 6、给Hadoop配置环境变量(完整的环境变量配置如下) 7、将环境变量发送给另外两个节点12#scp -r /etc/profile node-2:/etc#scp -r /etc/profile node-3:/etc 在发送完成后，都要使用source /etc/profile命令重新加载环境变量 8、启动集群 按顺序执行以下操作 在node-1上：1234567#zkServer.sh start#zkServer.sh status(要确保zookeeper服务已经启动)#hadoop-daemons.sh start journalnode (启动journalnode服务)#hdfs zkfc –formatZK(格式化zkfc,让在zookeeper中生成ha节点,只需在第一次启动集群是格式化即可)#hadoop namenode -format (格式化hdfs,也只在第一次启动集群时格式化)#hadoop-daemon.sh start namenode 在node-2上(设置node-2为备份的namenode,所以需要在node-2上启动namenode进程)：12 #hdfs namenode -bootstrapStandby(在第一次启动时执行)#hadoop-daemon.sh start namenode 启动datanode(任意节点都可以):1#hadoop-daemons.sh start datanode 在node-3上启动yarn；1#start-yarn.sh 启动zkfc服务(任意节点都可以):1#hadoop-daemons.sh start zkfc 然后就可以Web进行访问了 http://node-1:50070 http://node-2:50070]]></content>
      <categories>
        <category>大数据</category>
      </categories>
      <tags>
        <tag>Hadoop，HA</tag>
        <tag>Zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[MyCat分片]]></title>
    <url>%2F2017%2F04%2F12%2FMyCat%E5%88%86%E7%89%87%2F</url>
    <content type="text"><![CDATA[MyCat分片 MyCat简介 &emsp;&emsp;Mycat 背后是阿里曾经开源的知名产品——Cobar。 Cobar的核心功能和优势是MySQL数据库分片。&emsp;&emsp;Mycat 是基于 cobar 演变而来， 对 cobar 的代码进行了彻底的重构， 使用 NIO 重构了网络模块，并且优化了Buffer 内核， 增强了聚合，oin等基本特性， 同时兼容绝大多数数据库成为通用的数据库中间件。简单的说， MyCAT 就是： 一个新颖的数据库中间件产品支持 mysql 集群，或者mariadbcluster， 提供高可用性数据分片集群。你可以像使用 mysql 一样使用 mycat。 对于开发人员来说根本感觉不到mycat的存在。 环境需求 JDK : 1.7及以上版本 MySQL: mysql5.5以上版本 ###Mysql安装与启动步骤： 将MySQL的服务端和客户端安装包(RPM)上传到服务器 查询之前是否安装过MySQL# rpm -qa|grep mysql 卸载旧版本的MySQLrpm -e --nodeps XXX 安装服务端rpm -ivh MySQL-server-5.6.17-1.el6.x86_64.rpm 安装客户端rpm -ivh MySQL-client-5.6.17-1.el6.x86_64.rpm 启动MySQL服务service mysql start 登陆MySQL(刚安装的Mysql密码在/root/.mysql_secret)mysql -uroot -p密码 设置远程登录权限GRANT ALL PRIVILEGES ON *.* TO &#39;root&#39;@&#39;%&#39;IDENTIFIED BY &#39;123456&#39; WITH GRANT OPTION;flush privileges; ##MyCat分片 ####分片相关的概念 逻辑库(schema) &emsp;&emsp;前面一节讲了数据库中间件， 通常对实际应用来说， 并不需要知道中间件的存在， 业务开发人员只需要知道数据库的概念， 所以数据库中间件可以被看做是一个或多个数据库集群构成的逻辑库。(可以理解为一个保存了数据真实地址的映射集合)。 逻辑表(table) &emsp;&emsp;既然有逻辑库，那么就会有逻辑表，分布式数据库中，对应用来说，读写数据的表就是逻辑表。逻辑表，可以是数据切分后，分布在一个或多个分片库中，也可以不做数据切分，不分片，只有一个表构成。&emsp;分片表： 是指那些原有的很大数据的表，需要切分到多个数据库的表，这样， 每个分片都有一部分数据，所有分片构成了完整的数据。总而言之就是需要进行分片的表。&emsp;非分片表： 一个数据库中并不是所有的表都很大，某些表是可以不用进行切分的， 非分片是相对分片表来说的， 就是那些不需要进行数据切分的表。 分片节点(dateNode) &emsp;&emsp;数据切分后，一个大表被分到不同的分片数据库上面，每个表所在的数据库就是分片节点（dataNode)。 -节点主机(dateHost) &emsp;&emsp;数据分片后，每个分片节点(dateNode)不一定都会独占一台机器，同一台机器上可以有多个分片数据库，这样一个或多个分片节点所在的机器就是节点主机,为了规避单节点并发数限制，尽量将读写压力高的分片节点均衡放在不同的节点主机。 -分片规则(rule) &emsp;&emsp;一个大表被分成若干个分片表，就需要一定的规则，这样按照某种业务规则把数据分到某个分片的规则就是分片规则，数据切分选择选择合适的分片规则非常重要，将极大的避免后续数据处理的难度。 ###MyCat分片配置 #####配置schema.xml schema 标签用于定义MyCat实例中的逻辑库&emsp;Table 标签定义了MyCat中的逻辑表&emsp;rule 用于指定分片规则，auto-sharding-long 的分片规则是按ID值的范围进行分片,1-5000000为第1片,5000001-10000000 为第 2 片….&emsp;dataNode标签定义了MyCat中的数据节点，也就是数据分片。dataHost标签在mycat逻辑库中也是作为最底层的标签存在，直接定义了具体的数据库实例、读写分离配置和心跳语句。schema.xml配置如下：12345678910111213141516171819&lt;?xml version="1.0"?&gt;&lt;!DOCTYPE mycat:schema SYSTEM "schema.dtd"&gt;&lt;mycat:schema xmlns:mycat="http://org.opencloudb/"&gt;&lt;schema name="PINYOUGOUDB" checkSQLschema="false" sqlMaxLimit="100"&gt;&lt;table name="tb_test" dataNode="dn1,dn2,dn3" rule="auto-sharding-long" /&gt;&lt;/schema&gt; &lt;dataNode name="dn1" dataHost="localhost1" database="db1" /&gt; &lt;dataNode name="dn2" dataHost="localhost1" database="db2" /&gt; &lt;dataNode name="dn3" dataHost="localhost1" database="db3" /&gt; &lt;dataHost name="localhost1" maxCon="1000" minCon="10" balance="0" writeType="0" dbType="mysql" dbDriver="native" switchType="1" slaveThreshold="100"&gt;&lt;heartbeat&gt;select user()&lt;/heartbeat&gt;&lt;writeHost host="hostM1" url="localhost:3306" user="root"password="123456"&gt;&lt;/writeHost&gt;&lt;/dataHost&gt;&lt;/mycat:schema&gt; #####配置server.xmlserver.xml几乎保存了mycat需要的系统配置信息。最常见的是在此配置用户名、密码以及权限。server.xml配置如下：添加UTF-8字符集设置，否则存储中文会出现问号1&lt;property name="charset"&gt;utf8&lt;/property&gt; 修改user的设置，为TESTDB添加root、test用户12345678&lt;user name="test"&gt;&lt;property name="password"&gt;test&lt;/property&gt;&lt;property name="schemas"&gt;TESTDB&lt;/property&gt;&lt;/user&gt;&lt;user name="root"&gt;&lt;property name="password"&gt;123456&lt;/property&gt;&lt;property name="schemas"&gt;TESTDB&lt;/property&gt;&lt;/user&gt; #####MyCat分片测试进入逻辑库TESTDB# mysql -uroot -p123456 -P8066 -DTESTDBshow tables;会发现已经有表tb_test执行以下语句创建一个表12345CREATE TABLE tb_test (id BIGINT(20) NOT NULL,title VARCHAR(100) NOT NULL ,PRIMARY KEY (id)) ENGINE=INNODB DEFAULT CHARSET=utf8 在创建完成后，会对表td_test的结构进行初始化。查看物理数据库db1,db2,db3,会发现，每个物理数据库中都已经创建好了表tb_test。 然后就可以向逻辑库中的逻辑表中插入数据，数据会按照分片规则进行分片存储。 #####MyCat分片规则rule.xml用于定义分片规则 按主键范围分片(rang-long) 123456&lt;tableRule name="auto-sharding-long"&gt; &lt;rule&gt; &lt;columns&gt;id&lt;/columns&gt; &lt;algorithm&gt;rang-long&lt;/algorithm&gt; &lt;/rule&gt;&lt;/tableRule&gt; 配置文件详解： tableRule :用于定义固体某个表或某类表的分片规则名称 column：用于定义分片的列 algorithm：代表算法的名称 rang-long的定义： 1234&lt;function name="rang-long" class="org.opencloudb.route.function.AutoPartitionByLong"&gt; &lt;property name="mapFile"&gt;autopartition-long.txt&lt;/property&gt;&lt;/function&gt; autopartition-long.txt的定义12345# range start-end ,data node index# K=1000,M=10000.0-500M=0500M-1000M=11000M-1500M=2 一致性哈希murmur当我们需要将数据平均分在几个分区中，需要使用一致性hash规则1234567891011&lt;function name="murmur" class="org.opencloudb.route.function.PartitionByMurmurHash"&gt; &lt;property name="seed"&gt;0&lt;/property&gt;&lt;!-- 默认是 0 --&gt; &lt;property name="count"&gt;3&lt;/property&gt; &lt;!--要分片的数据库节点数量，必须指定，否则没法分片--&gt; &lt;property name="virtualBucketTimes"&gt;160&lt;/property&gt; &lt;!-- 一个实际的数据库节点被映射为这么多虚拟节点， 默认是 160 倍，也就是虚拟节点数是物理节点数的 160 倍 --&gt;&lt;!-- &lt;property name="weightMapFile"&gt;weightMapFile&lt;/property&gt; 节点的权重，没有指定权重的节点默认是1。以properties文件的格式填写， 以从 0 开始到 count-1的整数值也就是节点索引为key，以节点权重值为值。 所有权重值必须是正整数， 否则以 1 代替 --&gt;&lt;!-- &lt;property name="bucketMapPath"&gt;/etc/mycat/bucketMapPath&lt;/property&gt;用于测试时观察各物理节点与虚拟节点的分布情况， 如果指定了这个属性， 会把虚拟节点的 murmur hash 值与物理节点的映射按行输出到这个文件， 没有默认值， 如果不指定， 就不会输出任何东西 --&gt;&lt;/function&gt; 配置文件中表规则的定义：123456&lt;tableRule name="sharding-by-murmur"&gt; &lt;rule&gt; &lt;columns&gt;id&lt;/columns&gt; &lt;algorithm&gt;murmur&lt;/algorithm&gt; &lt;/rule&gt;&lt;/tableRule&gt; 在这里是按照id进行hash运算，然后确定数据的分片存储位置]]></content>
      <categories>
        <category>分布式</category>
      </categories>
      <tags>
        <tag>MayCat</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Maven中的依赖传递]]></title>
    <url>%2F2017%2F03%2F04%2FMaven%E4%B8%AD%E7%9A%84%E4%BE%9D%E8%B5%96%E4%BC%A0%E9%80%92%2F</url>
    <content type="text"><![CDATA[Maven中的依赖传递 一、依赖范围scope Maven因为执行一系列编译、测试、和部署等操作，在不同的操作下使用的classpath不同，依赖范围就是控制依赖与三种classpath(编译classpath、测试classpath、运行classpath)的关系。 一共有5种，compile(编译)、test(测试)、runtime(运行时)、provided、system不指定，则范围默认为compile。 compile：编译依赖范围，在编译、测试、运行时都需要。 test：测试依赖范围，测试时需要。编译和运行不需要。 runtime：运行时以来范围，测试和运行时需要，编译不需要。 provided：已提供依赖范围，编译和测试时需要。运行时不需要。 system：系统依赖范围。本地依赖，不在maven中央仓库。 二、依赖的传递A-&gt;B(compile) a依赖b &gt;b是A的编译依赖范围，在A编译、测试、运行时都依赖b B-&gt;C(compile) B依赖C &gt;c是B的编译依赖范围，在B编译、测试、运行时都依赖c 当在A中配置 &lt;dependency&gt; &lt;groupId&gt;com.B&lt;/groupId&gt; &lt;artifactId&gt;B&lt;/artifactId&gt; &lt;version&gt;1.0&lt;/version&gt; &lt;/dependency&gt; 则会自动导入c包。关系传递如下表： 由上表不难看出，项目A具体会不会导入B依赖的包c，取决于第一和第二依赖，但是依赖的范围不会超过第一直接依赖，即具体会不会引入c包，要看第一直接依赖的依赖范围。 三、以来冲突的调节A -&gt; B -&gt; C -&gt; X(1.0) A -&gt; D -&gt; X(2.0) 由于只能引入一个版本的包，此时Maven按照最短路径选择导入X(2.0). A -&gt; B -&gt; X(1.0) A -&gt; D -&gt; X(2.0) 路径长度一致，则优先选择第一个，此时导入X(1.0). 四、排除依赖A -&gt; B -&gt; C(1.0) 此时在A项目中，不想使用C(1.0)，而使用C(2.0) 则需要使用exclusion排除B对C(1.0)的依赖。并在A中引入C(2.0). pom.xml中配置 &lt;!--排除B对C的依赖--&gt; &lt;dependency&gt; &lt;groupId&gt;B&lt;/groupId&gt; &lt;artifactId&gt;B&lt;/artifactId&gt; &lt;version&gt;0.1&lt;/version&gt; &lt;exclusions&gt; &lt;exclusion&gt; &lt;groupId&gt;C&lt;/groupId&gt; &lt;artifactId&gt;C&lt;/artifactId&gt;&lt;!--无需指定要排除项目的 版本号--&gt; &lt;/exclusion&gt; &lt;/exclusions&gt; &lt;/dependency&gt; &lt;!---在A中引入C(2.0)--&gt; &lt;dependency&gt; &lt;groupId&gt;C&lt;/groupId&gt; &lt;artifactId&gt;C&lt;/artifactId&gt; &lt;version&gt;2.0&lt;/version&gt; &lt;/dependency&gt; 五、依赖关系的查看cmd进入工程根目录，执行 mvn dependency:tree 会列出依赖关系树及各依赖关系 mvn dependency:analyze 分析依赖关系]]></content>
      <categories>
        <category>Web</category>
      </categories>
      <tags>
        <tag>Maven</tag>
      </tags>
  </entry>
</search>
